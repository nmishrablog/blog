## AI/Wetware - apps {#h.p_1d3aKWHj2Rfb}

TL;DR – A master learning algorithm like the universal theory, will take much more time to be developed, and probably need a different architecture. Brute forcing ML/CNN/RNN/Simulation based learning on SoC built for these types of processes might have a higher chance for solving niche problems faster and cheaper.

##  AI is whatever hasn’t been done yet {#h.p_m6Bo62ou2ayT}

What do we mean when we say A.I ? we mean A.G.I i.e HAL 9000 or J.A.R.V.I.S. Most of the “Bots” we have today are not an A.G.I , they are very barely an A.S.I \(Artificial Specific Intelligence\) i.e. verticalized problem solving. These are possible due to three factors

1. **Cheap compute**
2. **Availability of large datasets**
3. **Algorithms that are available via API.**

They all \(Most of them \) focus on algorithms that take a huge amount of labelled data \(Structured or Unstructured\) and compare the current task to the previous best known solution.

This is almost a brute force approach and not at all how we humans actually learn anything. Most self driving cars learn to drive by driving\( either physically or in a simulation\) millions of miles and comparing what it did on a particular situation in the past ,to what it should do in the present.This is very resource intensive , **the human brain uses about a half a watt to do very impressive stuff.**

Scientist have adopted bio-mimicry to solve many of the larger scale physical problems of adhesion, color sensitivity, robotic motion etc. However there seems to be an inertia of doing similar stuff when it comes to computation. Reasons for this behavior might be –

1. **Physical complexity of the brain**
2. **Information gap of how simple synaptic firing leads to decision making i.e how synapses can be modeled by logic gates**
3. **Expensive wetware required for in-depth studies.**

Should we go after research on the forefront of neuroscience to understand the way to do A.I better? Yes, but we may not need it to solve some of the easier problems that are being worked on right now. We are very close to almost human like delivery of knowledge using the current techniques and with Open AI and chipmakers working for A.I SoC’s the near future will see greater improvements than semiconductor tech saw with Moore’s law.

I hypothesize that this will continue up to a certain point.The real exponential increase in delivering A.I will come from a different chip architecture and a different class of algorithms running on human scale resources.

Reference Links:

* Numenta Interview with Kara Swisher -http://www.recode.net/2016/6/27/12037248/artificial-intelligence-machine-learning-numenta-jeff-hawkins-donna-dubinsky-podcast
* Human-Level Artificial Intelligence?Be Serious! – Nils J. Nilsson – 
  _**ai**.stanford.edu/~nilsson/OnlinePubs-Nils/…/AIMag26-04-HLAI.pdf_
* Rule of Automation: Never ask the user for any information that you can autodetect, copy, or deduce -http://www.catb.org/esr/writings/taouu/taouu.html\#rule-automation
*  a16z Podcast: When Humanity Meets A.I. – http://a16z.com/2016/06/29/feifei-li-a16z-professor-in-residence/
* AI, Deep Learning, and Machine Learning: A Primer – http://a16z.com/2016/06/10/ai-deep-learning-machines/
* Experts explain the biggest obstacles to creating human-like robots – http://www.techinsider.io/biggest-challenges-human-artificial-intelligence-2016-2
* https://en.wikipedia.org/wiki/History\_of\_artificial\_intelligence \(C’mon its 2016 Wikipedia should be some sort of a reference\)

  


